{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data into memory and spliting them into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('train.csv')\n",
    "train_text, val_text, train_label, val_label = train_test_split(train_pd['text'], train_pd['target'],\n",
    "                                                                test_size = 0.3, random_state = 2020)\n",
    "train_data = [{'text' : text, 'target' : label} for text, label in zip(train_text, train_label)]\n",
    "val_data = [{'text' : text, 'target' : label} for text, label in zip(val_text, val_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining = 'bert-base-uncased'\n",
    "encoder = BertTokenizer.from_pretrained(pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class disaster_data(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, encoder):\n",
    "        super(disaster_data, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.data = dataset\n",
    "        self.text = [row['text'] for row in self.data]\n",
    "        self.labels = [row['target'] for row in self.data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        holder = {}\n",
    "        encoded = encoder.batch_encode_plus([self.text[index]], max_length = 30, truncation = True, pad_to_max_length = True)\n",
    "        holder['embedding'] = torch.tensor(encoded['input_ids']).squeeze()\n",
    "        holder['mask'] = torch.tensor(encoded['attention_mask']).squeeze()\n",
    "        holder['label'] = float(self.labels[index])\n",
    "        return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretraining, inter_size = [1024, 16]):\n",
    "        super(NLPModel, self).__init__()\n",
    "        assert type(inter_size) is list, 'inter_size must be list'\n",
    "        assert len(inter_size) == 2, 'The Length of inter_size must be 2'\n",
    "        self.Bert = BertModel.from_pretrained(pretraining)\n",
    "        self.hidden_size = self.Bert.config.hidden_size\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, inter_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(inter_size[0], inter_size[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(inter_size[1], 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        cls_emb = self.Bert(x, mask)[0][:, 0, :]\n",
    "        prediction = self.cls(cls_emb)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cls = 0.0001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = disaster_data(train_data, encoder)\n",
    "valSet = disaster_data(val_data, encoder)\n",
    "trainLoader = DataLoader(trainSet, batch_size = 64, shuffle = True)\n",
    "valLoader = DataLoader(valSet, batch_size = 64, shuffle = False)\n",
    "gpu = torch.device('cuda')\n",
    "model = NLPModel(pretraining).to(gpu)\n",
    "#class_weights = compute_class_weight('balanced', classes = [0, 1], y = [row['target'] for row in train])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.cls.parameters(), lr_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.Bert.parameters():\n",
    "    param.required_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(trainLoader):\n",
    "        \n",
    "        emb = batch['embedding'].to(gpu)\n",
    "        label = batch['label'].to(gpu)\n",
    "        mask = batch['mask'].to(gpu)\n",
    "        \n",
    "        output = model(emb, mask).double()\n",
    "        \n",
    "        loss = criterion(output, label[:, None])\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return round(total_loss / len(trainLoader), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    pred = torch.tensor([])\n",
    "    labelSet = torch.tensor([])\n",
    "    \n",
    "    for batch in valLoader:\n",
    "        \n",
    "        emb = batch['embedding'].to(gpu)\n",
    "        label = batch['label'].to(gpu)\n",
    "        mask = batch['mask'].to(gpu)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(emb, mask).double()\n",
    "            \n",
    "            loss = criterion(output, label[:, None])\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.cat([pred, torch.round(torch.sigmoid(output)).squeeze().float().cpu()])\n",
    "            labelSet = torch.cat([labelSet, label.float().cpu()])\n",
    "            \n",
    "    accuracy = torch.eq(pred, labelSet).sum().item() / len(valLoader)\n",
    "    return pred, round(total_loss / len(valLoader), 5), round(accuracy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    best_eval_loss = float('inf')\n",
    "    train_loss = train()\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    print(f'Training loss : {train_loss}')\n",
    "    print('Evaluating...')\n",
    "    pred, eval_loss, accuracy = evaluate()\n",
    "    print(f'Validation loss : {eval_loss} | Accuracy : {accuracy}')\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        print(f'New eval loss was generated, the current best one is {best_eval_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
