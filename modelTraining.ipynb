{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- How to fine-tune BERT for text classification https://arxiv.org/pdf/1905.05583.pdf\n",
    "- BERT fine-tuning tutorial with Pytorch https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data into memory and spliting them into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('train.csv')\n",
    "train_text, val_text, train_label, val_label = train_test_split(train_pd['text'], train_pd['target'],\n",
    "                                                                test_size = 0.2, random_state = 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    compiler = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    \n",
    "    output = compiler.sub('', text)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def remove_html(text):\n",
    "    compiler = re.compile(r'<.*?>')\n",
    "    \n",
    "    output = compiler.sub('', text)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    compiler = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    output = compiler.sub('', text)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    compiler = re.compile(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\")\n",
    "    output = compiler.sub('', text)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def data_cleaning(text):\n",
    "    text = remove_url(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_hashtag(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [{'text' : text, 'target' : label} for text, label in zip(train_text, train_label)]\n",
    "val_data = [{'text' : text, 'target' : label} for text, label in zip(val_text, val_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining = 'roberta-base'\n",
    "encoder = RobertaTokenizer.from_pretrained(pretraining, do_lower_case = True)\n",
    "#pretraining = 'bert-base-uncased'\n",
    "#encoder = BertTokenizer.from_pretrained(pretraining, do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class disaster_data(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, encoder):\n",
    "        super(disaster_data, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.data = dataset\n",
    "        self.text = [row['text'] for row in self.data]\n",
    "        self.labels = [row['target'] for row in self.data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        holder = {}\n",
    "        encoded = encoder.batch_encode_plus([self.text[index]], max_length = 30, truncation = True, pad_to_max_length = True)\n",
    "        holder['embedding'] = torch.tensor(encoded['input_ids']).squeeze()\n",
    "        holder['mask'] = torch.tensor(encoded['attention_mask']).squeeze()\n",
    "        holder['label'] = float(self.labels[index])\n",
    "        return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretraining):\n",
    "        super(NLPModel, self).__init__()\n",
    "        #self.Bert = BertModel.from_pretrained(pretraining)\n",
    "        self.Bert = RobertaModel.from_pretrained(pretraining)\n",
    "        self.hidden_size = self.Bert.config.hidden_size\n",
    "        self.cls = nn.Linear(self.hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        cls_emb = self.Bert(x, mask)[1]\n",
    "        prediction = self.cls(self.dropout(cls_emb))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "lr_bert_decay = 0.95\n",
    "epochs = 3\n",
    "path = 'model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = disaster_data(train_data, encoder)\n",
    "valSet = disaster_data(val_data, encoder)\n",
    "trainLoader = DataLoader(trainSet, batch_size = 16, shuffle = True)\n",
    "valLoader = DataLoader(valSet, batch_size = 16, shuffle = False)\n",
    "gpu = torch.device('cuda')\n",
    "model = NLPModel(pretraining).to(gpu)\n",
    "lr_bert_config = []\n",
    "for index in range(len(model.Bert.encoder.layer)):\n",
    "    holder = {'params' : model.Bert.encoder.layer[-(index + 1)].parameters(),\n",
    "              'lr' : lr * (lr_bert_decay ** index)}\n",
    "    lr_bert_config.append(holder)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction = 'sum')\n",
    "optimizer_cls = AdamW(model.cls.parameters(), lr)\n",
    "optimizer_bert = AdamW(lr_bert_config)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optimizer_bert,\n",
    "                                            num_warmup_steps = 150,\n",
    "                                            num_training_steps = len(trainLoader) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(trainLoader):\n",
    "        \n",
    "        emb = batch['embedding'].to(gpu)\n",
    "        label = batch['label'].to(gpu)\n",
    "        mask = batch['mask'].to(gpu)\n",
    "        \n",
    "        output = model(emb, mask).double()\n",
    "        \n",
    "        loss = criterion(output, label[:, None])\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer_bert.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_bert.step()\n",
    "        optimizer_cls.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "    return round(total_loss / len(trainSet), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    pred = []\n",
    "    labelSet = []\n",
    "    \n",
    "    for batch in valLoader:\n",
    "        \n",
    "        emb = batch['embedding'].to(gpu)\n",
    "        label = batch['label'].to(gpu)\n",
    "        mask = batch['mask'].to(gpu)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(emb, mask).double()\n",
    "            \n",
    "            loss = criterion(output, label[:, None])\n",
    "            total_loss += loss.item()\n",
    "            pred.extend(output.cpu().detach().numpy().tolist())\n",
    "            labelSet.extend(label.cpu().detach().numpy().tolist())\n",
    "    \n",
    "    pred = np.array(pred) >= 0.5\n",
    "    accuracy = accuracy_score(labelSet, pred)\n",
    "    \n",
    "    return pred, round(total_loss / len(valSet), 5), round(accuracy, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    log = []\n",
    "    best_eval_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train()\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "        print(f'Training loss : {train_loss}')\n",
    "        print('Evaluating...')\n",
    "        pred, eval_loss, accuracy = evaluate()\n",
    "        print(f'Validation loss : {eval_loss} | Accuracy : {accuracy}')\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(f'New eval loss was generated, the current best one is {best_eval_loss}')\n",
    "        log.append({'epoch' : epoch + 1,\n",
    "                    'train_loss' : train_loss,\n",
    "                    'eval_loss' : eval_loss,\n",
    "                    'best_eval_loss' : best_eval_loss})\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(log):\n",
    "    \n",
    "    timestamp = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    for dic in log:\n",
    "        timestamp.append(dic['epoch'])\n",
    "        train_error.append(dic['train_loss'])\n",
    "        val_error.append(dic['eval_loss'])\n",
    "        \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    train_error_plot = sns.lineplot(x = timestamp, y = train_error)\n",
    "    train_error_plot.set(xlabel = 'Epochs', ylabel = 'Training Error')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    val_error_plot = sns.lineplot(x = timestamp, y = val_error)\n",
    "    val_error_plot.set(xlabel = 'Epochs', ylabel = 'Validation Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = run()\n",
    "plot_error(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disaster_test_set(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, encoder):\n",
    "        super(Disaster_test_set, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.data = dataset\n",
    "        self.text = [text for text in self.data['text']]\n",
    "        self.id = dataset['id']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        holder = {}\n",
    "        encoded = encoder.batch_encode_plus([self.text[index]], max_length = 30, truncation = True, pad_to_max_length = True)\n",
    "        holder['id'] = self.id[index]\n",
    "        holder['embedding'] = torch.tensor(encoded['input_ids']).squeeze()\n",
    "        holder['mask'] = torch.tensor(encoded['attention_mask']).squeeze()\n",
    "        return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd = pd.read_csv('test.csv')\n",
    "testSet = Disaster_test_set(test_pd, encoder)\n",
    "testLoader = DataLoader(testSet, batch_size = 32, shuffle = False)\n",
    "model = NLPModel(pretraining)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model = model.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    \n",
    "    ids = []\n",
    "    prediction = []\n",
    "    for batch in testLoader:\n",
    "        emb = batch['embedding'].to(gpu)\n",
    "        mask = batch['mask'].to(gpu)\n",
    "        ids.extend(batch['id'].tolist())\n",
    "        \n",
    "        output = model(emb, mask).squeeze().detach().cpu()\n",
    "        output = np.array(output) >= 0.5\n",
    "        prediction.extend(output.astype(int).tolist())\n",
    "    return ids, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id, pred = predict()\n",
    "testFrame = pd.DataFrame.from_dict({'id' : id,\n",
    "                                    'target' : pred})\n",
    "testFrame.to_csv('submisson.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
